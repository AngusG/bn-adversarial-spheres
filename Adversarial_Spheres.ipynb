{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yang et al., 'Mean Field Theory of Batch Norm', ICLR 2019\n",
    "Reproduces experiment in Figure 2/6. \n",
    "https://openreview.net/pdf?id=SyMDXnCcF7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slicing explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# to normalize without for-loop\n",
    "from torch import FloatTensor\n",
    "from torch.nn.functional import normalize\n",
    "\n",
    "\n",
    "N=int(1e8)\n",
    "d=3\n",
    "save_dir = './'\n",
    "\n",
    "# generate datapoints distributed uniformly on a (d-1)-sphere with radius 1\n",
    "x = np.random.multivariate_normal(np.zeros(d), np.eye(d), N)\n",
    "x_normed = normalize(FloatTensor(x), dim=1, eps=1e-16).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A (d-1)-sphere is described by the equation of d coordinates and the radius R:\n",
    "`\\sum\\limits_{i=1}^d x_i^2 = R^2`\n",
    "\n",
    "A slice of a (d-1)-sphere is described by 2 coordinates constrained as:\n",
    "`x_1^2 + x_2^2 = R^2 - \\sum\\limits_{i=3}^d x_i^2`.\n",
    "\n",
    "Thus, the points from the sphere corresponding to a slice in the x_1-x_2-plane obey \n",
    "`\\sum\\limits_{i=3}^d x_i^2 = const`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eff=1e-6\n",
    "set_const = 0.5\n",
    "mask=(abs(np.sum(x_normed[:,2:]**2, axis=1)-set_const)<eff)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6,6))\n",
    "ax.scatter(x_normed[mask,0], x_normed[mask,1])\n",
    "fig.savefig(save_dir+'slice.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# for mutual information\n",
    "#from mnist.ft_utils import mi\n",
    "from numpy.linalg import det, inv\n",
    "\n",
    "# for advex\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# for 3D viz of decision boundary\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# for linear model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = 1.3\n",
    "LABEL_OUTER_SPHERE = 1\n",
    "\n",
    "\n",
    "def generate_clean_batch(batch_size, d):\n",
    "    \"\"\"\n",
    "    Returns a mini-batch of synthetic spheres in online setting \n",
    "    @param batch_size of mini-batch\n",
    "    @param d: dimensionality of the spheres\n",
    "    \"\"\"\n",
    "    x = np.random.multivariate_normal(np.zeros(d), np.eye(d), batch_size)\n",
    "\n",
    "    x_train = np.zeros((batch_size, d))\n",
    "    y_train = np.zeros((batch_size, 1))\n",
    "\n",
    "    x_shuff = np.zeros((batch_size, d))\n",
    "    y_shuff = np.zeros((batch_size, 1))\n",
    "\n",
    "    euclidean_norm = np.linalg.norm(x, axis=1)\n",
    "\n",
    "    # outer sphere, radius R\n",
    "    for i in range(batch_size // 2):\n",
    "        x_train[i, :] = R * x[i, :] / euclidean_norm[i]\n",
    "\n",
    "    # inner sphere, radius 1\n",
    "    for i in range(batch_size // 2, batch_size):\n",
    "        x_train[i, :] = x[i, :] / euclidean_norm[i]\n",
    "\n",
    "    # assign training labels\n",
    "    y_train[:batch_size // 2] = LABEL_OUTER_SPHERE\n",
    "\n",
    "    return (x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=10000 # train on 10k pts\n",
    "test_N=100000 # eval on 1M pts\n",
    "d=30\n",
    "train_x, train_y = generate_clean_batch(N, d)\n",
    "val_x, val_y = generate_clean_batch(N, d)\n",
    "test_x, test_y = generate_clean_batch(test_N, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train_y.reshape(-1)\n",
    "val_y = val_y.reshape(-1)\n",
    "test_y = test_y.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmin(train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular, when we sampled 10,000 random points on the inner sphere, the nearest pair was distance 1.25 away\n",
    "from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(train_x[train_y==0], 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#i = 0\n",
    "idx = N // 4\n",
    "print(np.linalg.norm(val_x[val_y==0][:idx] - val_x[val_y==0][idx:], 2, axis=1).min())\n",
    "idx = test_N // 4\n",
    "print(np.linalg.norm(test_x[test_y==0][:idx] - test_x[test_y==0][idx:], 2, axis=1).min())\n",
    "#train_x[train_y==0][i + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idx = N // 4\n",
    "#np.linalg.norm(X_val_inner[:idx] - X_val_inner[idx:], 2, axis=1).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adv_x[adv_y==0].shape\n",
    "#test_N // 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(6, 3))\n",
    "colors_txt = ['red', 'blue']\n",
    "ax1.scatter(train_x[:, 0], train_x[:, 1], c=train_y, alpha=0.5, \n",
    "            cmap=mpl.colors.ListedColormap(colors_txt))\n",
    "ax1.set_title(\"Train\")\n",
    "ax2.scatter(test_x[:, 0], test_x[:, 1], c=test_y, alpha=0.5, \n",
    "            cmap=mpl.colors.ListedColormap(colors_txt))\n",
    "ax2.set_title(\"Test\")\n",
    "\"\"\"\n",
    "#fig.savefig('AdversarialSpheres.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = torch.nn.Softmax(dim=0)\n",
    "batch_softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "seed = 1234\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "X_train = torch.tensor(train_x, dtype=torch.float)\n",
    "Y_train = torch.tensor(train_y, dtype=torch.long)\n",
    "\n",
    "train_batch_size = 50\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, Y_train)\n",
    "train_loader_noshuffle = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset, batch_size=1000, shuffle=False)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "# Val\n",
    "X_val = torch.tensor(val_x, dtype=torch.float)\n",
    "Y_val = torch.tensor(val_y, dtype=torch.long)\n",
    "\n",
    "val_dataset = torch.utils.data.TensorDataset(X_val, Y_val)\n",
    "val_loader_noshuffle = torch.utils.data.DataLoader(\n",
    "    dataset=val_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "# Val inner sphere only\n",
    "X_val_inner = torch.tensor(val_x[N // 2:], dtype=torch.float)\n",
    "Y_val_inner = torch.tensor(val_y[N // 2:], dtype=torch.long)\n",
    "val_inner_dataset = torch.utils.data.TensorDataset(X_val_inner, Y_val_inner)\n",
    "val_inner_loader_noshuffle = torch.utils.data.DataLoader(\n",
    "    dataset=val_inner_dataset, batch_size=N // 2, shuffle=False)\n",
    "\n",
    "# Test\n",
    "X_test = torch.tensor(test_x, dtype=torch.float)\n",
    "Y_test = torch.tensor(test_y, dtype=torch.long)\n",
    "\n",
    "# create datasets\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, Y_test)\n",
    "test_loader_noshuffle = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset, batch_size=N // 10, shuffle=False) # 100k / 10 = 10k\n",
    "\n",
    "# Tst inner sphere only\n",
    "X_tst_inner = torch.tensor(test_x[test_N // 2:], dtype=torch.float)\n",
    "Y_tst_inner = torch.tensor(test_y[test_N // 2:], dtype=torch.long)\n",
    "tst_inner_dataset = torch.utils.data.TensorDataset(X_tst_inner, Y_tst_inner)\n",
    "tst_inner_loader_noshuffle = torch.utils.data.DataLoader(\n",
    "    dataset=tst_inner_dataset, batch_size=N // 20, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = []\n",
    "def hook(module, input, output):\n",
    "    activations.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLUNetwork(nn.Module):\n",
    "    def __init__(self, input_size, num_layers, num_units, num_classes, do_batch_norm=False):\n",
    "        super(ReLUNetwork, self).__init__()\n",
    "        self.do_batch_norm = do_batch_norm\n",
    "        self.input_layer = nn.Linear(input_size, num_units, bias=True)\n",
    "        self.features = self._make_layers(num_layers, num_units)\n",
    "        self.classifier = nn.Linear(num_units, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Forward pass, returns outputs of each layer. Use last out (final) for backprop!\"\"\"\n",
    "        out = self.input_layer(inputs)\n",
    "        out = self.features(out)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "    \n",
    "    def _make_layers(self, num_layers, num_units):\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            if self.do_batch_norm:\n",
    "                layers += [nn.BatchNorm1d(num_units, momentum=None),\n",
    "                           nn.Linear(num_units, num_units, bias=True),\n",
    "                           nn.ReLU()]\n",
    "            else:\n",
    "                layers += [nn.Linear(num_units, num_units, bias=True),\n",
    "                           nn.ReLU()]\n",
    "        return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_units = 1000\n",
    "num_layers = 60\n",
    "do_batch_norm = True\n",
    "N_CLASSES = 2\n",
    "#device = torch.device('cpu')\n",
    "device = torch.device('cuda:0')\n",
    "model = ReLUNetwork(train_x.shape[1], num_layers, num_units, N_CLASSES, do_batch_norm=do_batch_norm).to(device)\n",
    "#model = LinearNetwork(train_x.shape[1], num_layers, num_units, N_CLASSES, do_batch_norm=do_batch_norm).to(device)\n",
    "loss_fnct = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "L = num_layers * 3 # ReLUNetwork\n",
    "register_idx = 0\n",
    "\n",
    "for i in range(L):\n",
    "    if i == register_idx:\n",
    "        j += 1\n",
    "        register_idx += 3\n",
    "        print('%d %s' % (j, model.features[i]))\n",
    "        model.features[i].register_forward_hook(hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations=[]\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        out = model(inputs)\n",
    "        break\n",
    "print(len(activations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.norm(p=2, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#activations[0].shape\n",
    "#for i in range(num_layers):\n",
    "#    print(i, activations[i].norm(p=2, dim=1).var())\n",
    "plt.hist(activations[38].norm(p=2, dim=1).detach().cpu().numpy())\n",
    "#print(activations[0].norm(p=2, dim=1).max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_type = ['trn', 'tst']\n",
    "lss = {}    # cross-entropy loss\n",
    "acc = {}    # prediction accuracy\n",
    "lss['trnmb'] = [] # one entry per minibatch\n",
    "\n",
    "for dst in dataset_type:\n",
    "    lss[dst] = [] # one entry per epoch\n",
    "    acc[dst] = [] # one entry per epoch\n",
    "        \n",
    "total_step = len(train_loader)\n",
    "\n",
    "max_epochs = 20\n",
    "for epoch in range(max_epochs):\n",
    "    model.train()\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fnct(outputs, labels)\n",
    "        lss['trnmb'].append(loss.item()) # record training loss\n",
    "        # backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # record train loss (avg value over mini-batches)\n",
    "    lss['trn'].append(np.mean(lss['trnmb'][-total_step:]))\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for inputs, labels in train_loader_noshuffle:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        train_acc = float(correct) / total\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        test_mb_loss = 0\n",
    "        for inputs, labels in val_loader_noshuffle:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            test_mb_loss += loss_fnct(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        test_acc = float(correct) / total\n",
    "        lss['tst'].append(test_mb_loss / len(val_loader_noshuffle))\n",
    "\n",
    "        print('Epoch [{}/{}], Loss: {:.5f} (Train), {:.5f} (Val); Acc: {:.5f} (Train), {:.5f} (Val)'\n",
    "              .format(epoch, max_epochs, lss['trn'][-1], lss['tst'][-1], train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "model.eval()\n",
    "for inputs, labels in test_loader_noshuffle:\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    outputs = model(inputs)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct_idx = predicted == labels \n",
    "    correct += (correct_idx).sum().item()\n",
    "test_acc = float(correct) / total\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "d_err_min = 10\n",
    "model.eval()\n",
    "for inputs, labels in val_inner_loader_noshuffle:\n",
    "    \n",
    "    labels = labels.to(device)\n",
    "    inputs = inputs.to(device)\n",
    "    advex = inputs.clone()\n",
    "    \n",
    "    for i in range(100):\n",
    "        advex = fgsm_l2(model, advex, labels, 0.1)\n",
    "        # project back on to manifold\n",
    "        advex /= torch.unsqueeze(advex.norm(p=2, dim=1), dim=1)\n",
    "        advex[labels==LABEL_OUTER_SPHERE] *= R\n",
    "        \n",
    "    outputs = model(advex)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct_idx = predicted == labels \n",
    "    correct += (correct_idx).sum().item()\n",
    "    \n",
    "    # compute nearest error on inner sphere\n",
    "    d_err = (inputs[correct_idx==0] - advex[correct_idx==0]).norm(p=2, dim=1).min().item()\n",
    "    if d_err < d_err_min:\n",
    "        d_err_min = d_err\n",
    "    \n",
    "test_acc = float(correct) / total\n",
    "print(test_acc, d_err_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tst_inner_loader_noshuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "model.eval()\n",
    "for inputs, labels in tst_inner_loader_noshuffle:\n",
    "    labels = labels.to(device)\n",
    "    inputs = inputs.to(device)\n",
    "    outputs = model(inputs)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct_idx = predicted == labels \n",
    "    correct += (correct_idx).sum().item()    \n",
    "test_acc = float(correct) / total\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correct_idx[10] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "d_err_min = 10\n",
    "model.eval()\n",
    "for i, (inputs, labels) in enumerate(val_inner_loader_noshuffle):\n",
    "    \n",
    "    labels = labels.to(device)\n",
    "    inputs = inputs.to(device)\n",
    "    advex = inputs.clone()\n",
    "    \n",
    "    for j in range(1000):\n",
    "        advex = fgsm_l2(model, advex, labels, 0.01)\n",
    "        # project back on to manifold\n",
    "        advex /= torch.unsqueeze(advex.norm(p=2, dim=1), dim=1)\n",
    "        #advex[labels==LABEL_OUTER_SPHERE] *= R\n",
    "        \n",
    "        outputs = model(advex)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct_idx = predicted == labels \n",
    "        correct += (correct_idx).sum().item()\n",
    "        \n",
    "        if (correct_idx == 0).sum() > 0:\n",
    "            # compute nearest error on inner sphere\n",
    "            d_err = (inputs[correct_idx==0] - advex[correct_idx==0]).norm(p=2, dim=1).min().item()\n",
    "            if d_err < d_err_min:\n",
    "                print('itr %d got d = %f' % (j, d_err))\n",
    "                d_err_min = d_err\n",
    "            break\n",
    "    print('%d of %d' % (i, len(val_inner_loader_noshuffle)))\n",
    "    \n",
    "test_acc = float(correct) / total\n",
    "print(test_acc, d_err_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "#d_err_min = 10\n",
    "model.eval()\n",
    "for inputs, labels in test_loader_noshuffle:\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    for i in range(100):\n",
    "        inputs = fgsm_l2(model, inputs, labels, 0.1)\n",
    "        # project back on to manifold\n",
    "        inputs /= torch.unsqueeze(inputs.norm(p=2, dim=1), dim=1)\n",
    "        inputs[labels==LABEL_OUTER_SPHERE] *= R\n",
    "    outputs = model(inputs)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "test_acc = float(correct) / total\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (cln_inputs, labels) in enumerate(test_loader_noshuffle):\n",
    "    cln_inputs = cln_inputs.to(device)\n",
    "    labels = labels.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute d(E) - distance to nearest error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((cln_inputs[labels==0] - inputs[labels==0]).norm(p=2, dim=1).min().item())\n",
    "print((cln_inputs[labels==1] - inputs[labels==1]).norm(p=2, dim=1).min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "def fgsm_l2(model, im, labels, eps):\n",
    "    \"\"\"Evaluate model on FGM.\"\"\"\n",
    "    x_ = Variable(im, requires_grad=True)\n",
    "    red_ind = list(np.arange(1, len(x_.shape)))\n",
    "    loss = criterion(model(x_), labels)\n",
    "    loss.backward()\n",
    "    loss_grad = x_.grad.data.clone()\n",
    "    square = torch.max(torch.FloatTensor([1e-12]).to(device),  # to prevent div by zero\n",
    "                       torch.sum(loss_grad**2, dim=red_ind, keepdim=True))\n",
    "    normalized_loss_grad = loss_grad / torch.sqrt(square)\n",
    "    adv_img = x_.detach() + (eps * normalized_loss_grad).to(device)\n",
    "    return adv_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inputs[labels==1].norm(2))\n",
    "print(inputs[labels==0].norm(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(10, 3))\n",
    "\n",
    "colors_txt = ['red', 'blue']\n",
    "ax1.scatter(train_x[:, 0], train_x[:, 1], c=train_y, alpha=0.5, \n",
    "            cmap=mpl.colors.ListedColormap(colors_txt))\n",
    "ax1.set_title(\"Train\")\n",
    "\n",
    "ax2.scatter(test_x[:, 0], test_x[:, 1], c=test_y, alpha=0.5, \n",
    "            cmap=mpl.colors.ListedColormap(colors_txt))\n",
    "ax2.set_title(\"Test\")\n",
    "\n",
    "inputs_np = inputs.detach().cpu().numpy()\n",
    "labels_np = labels.detach().cpu().numpy()\n",
    "\n",
    "ax3.scatter(inputs_np[:, 0], inputs_np[:, 1], c=labels_np, alpha=0.5, \n",
    "            cmap=mpl.colors.ListedColormap(colors_txt))\n",
    "ax3.set_title(\"Adversarial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Field Theory - Figure 2.\n",
    "\n",
    "Batch norm leads to a chaotic input-output map with increasing depth. A linear network with batch norm is shown acting on two minibatches of size 64 after random orthogonal initialization. The datapoints in the minibatch are chosen to form a 2d circle in input space, except for one datapoint that is perturbed separately in each minibatch (leftmost datapoint at input layer 0). Because the network is linear, for a given minibatch it performs an affine transformation on its inputsâ€“ a circle in input space remains an ellipse throughout the network.  However, due to batch norm the coefficients of that affine transformation change nonlinearly as the datapoints in the minibatchare changed.(a) Each pane shows a scatterplot of activations at a given layer for all datapointsin the minibatch, projected onto the top two PCA directions. PCA directions are computed using the concatenation of the two minibatches. Due to the batch norm  nonlinearity, mini-batches that are nearly identical in input space grow increasingly dissimilar with depth.  Intuitively, this chaotic input-output map can be understood as the source of exploding gradients when batch norm is applied to very deep networks, since very small changes in an input correspond to very large movements in network outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations=[]\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in train_loader_noshuffle:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        out = model(inputs)\n",
    "        break\n",
    "print(len(activations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "register_idx = 3\n",
    "if do_batch_norm:\n",
    "    L = num_layers * 3 # ReLUNetwork\n",
    "    register_idx = 1\n",
    "else:\n",
    "    L = num_layers\n",
    "for i in range(L):\n",
    "    if i == register_idx:\n",
    "        j += 1\n",
    "        register_idx += 3\n",
    "        print('%d %s' % (j, model.features[i]))\n",
    "        model.features[i].register_forward_hook(hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = []\n",
    "\n",
    "#model.train() #\n",
    "model.eval()\n",
    "\n",
    "for i, (inputs, labels) in enumerate(train_loader_noshuffle):\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    if i == 0:\n",
    "        x_b0 = inputs.detach().cpu().numpy()\n",
    "        y_b0 = labels.detach().cpu().numpy()\n",
    "        pred = model(inputs)\n",
    "    elif i == 1:\n",
    "        x_b1 = inputs.detach().cpu().numpy()\n",
    "        y_b1 = labels.detach().cpu().numpy()\n",
    "        pred = model(inputs)\n",
    "    else:\n",
    "        break        \n",
    "print(len(activations))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
